{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.ensemble as ske\n",
    "\n",
    "from bounos.Analyses.Weight import summed_outliers_per_weight\n",
    "\n",
    "def categorise_dataframe(df):\n",
    "    # Categories work better as indexes\n",
    "    for obj_key in df.keys()[df.dtypes == object]:\n",
    "        try:\n",
    "            df[obj_key] = df[obj_key].astype('category')\n",
    "        except TypeError:\n",
    "            print(\"Couldn't categorise {}\".format(obj_key))\n",
    "            pass\n",
    "    return df\n",
    "\n",
    "def non_zero_rows(df):\n",
    "    return df[~(df==0).all(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The questions we want to answer are:\n",
    "1. What metrics differentiate between what behaviours\n",
    "2. Do these metrics cross over domains (i.e. comms impacting behaviour etc)\n",
    "\n",
    "To answer these questions we first have to manipulate the raw dataframe to be weight-indexed with behaviour(`var`) keys on the perspective from the observer to the (potential) attacker in a particular run (summed across the timespace)\n",
    "\n",
    "While this analysis simply sums both the upper and lower outliers, **this needs extended/readdressed**\n",
    "\n",
    "# IMPORTANT \n",
    "The July 3rd Simulation Run had a small mistake where the Ran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "observer = 'Bravo'\n",
    "target = 'Alfa'\n",
    "n_nodes = 6\n",
    "n_metrics = 9\n",
    "\n",
    "results_path = \"/home/bolster/src/aietes/results/Malicious Behaviour Trust Comparison-2015-07-03-16-45-26\"\n",
    "results_path = \"/home/bolster/src/aietes/results/Malicious Behaviour Trust Comparison-2015-07-20-17-47-53\"\n",
    "shared_h5_path = '/dev/shm/shared.h5'\n",
    "\n",
    "with pd.get_store(shared_h5_path) as store:\n",
    "    joined_target_weights = store.get('joined_target_weights')\n",
    "    joined_feats = store.get('joined_feats')\n",
    "    comms_only_feats = store.get('comms_only_feats')\n",
    "    phys_only_feats = store.get('phys_only_feats')\n",
    "    comms_only_weights = store.get('comms_only_weights')\n",
    "    phys_only_weights = store.get('phys_only_weights')\n",
    "\n",
    "joined_feat_weights = categorise_dataframe(non_zero_rows(joined_feats).T)\n",
    "comms_feat_weights = categorise_dataframe(non_zero_rows(comms_only_feats).T)\n",
    "phys_feat_weights = categorise_dataframe(non_zero_rows(phys_only_feats).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weight_df = (joined_target_weights).apply(lambda df: (df-joined_target_weights['CombinedTrust']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "target = 'CombinedBadMouthingPowerControl'\n",
    "df = weight_df[target].reset_index()\n",
    "data = df.drop(target, axis=1).values\n",
    "labels = df[target].values\n",
    "etr = ske.ExtraTreesRegressor(n_jobs=4, n_estimators=512)\n",
    "rtr = ske.RandomForestRegressor(n_jobs=4, n_estimators=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.647 -0.719 -1.053] DescribeResult(nobs=3, minmax=(-2.6469238688140218, -0.71899058657295978), mean=-1.4729789834756615, variance=1.0615042743276055, skewness=-0.6244115587804627, kurtosis=-1.4999999999999996)\n",
      "[-2.604 -0.714 -0.952] DescribeResult(nobs=3, minmax=(-2.6041888415286918, -0.71355304141383746), mean=-1.4230939712018928, variance=1.0603982712971967, skewness=-0.6648550723268009, kurtosis=-1.5000000000000004)\n",
      "[-3.028 -0.821 -1.136] DescribeResult(nobs=3, minmax=(-3.0280587211601571, -0.82062555015657546), mean=-1.6616597054254643, variance=1.4251964785709224, skewness=-0.6518939832871349, kurtosis=-1.5000000000000004)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, cross_validation\n",
    "linr = linear_model.LinearRegression()\n",
    "for reg in [etr,rtr,linr]:\n",
    "    scores = cross_validation.cross_val_score(reg, data, labels, scoring='mean_squared_error', n_jobs=4)\n",
    "    print scores, sp.stats.describe(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-2.962 -0.959 -1.107] DescribeResult(nobs=3, minmax=(-2.9621733228118092, -0.95888797451615915), mean=-1.675997737110861, variance=1.2461649799446983, skewness=-0.6931416166960097, kurtosis=-1.4999999999999998)\n",
      "[-2.978 -1.074 -1.221] DescribeResult(nobs=3, minmax=(-2.9775661956890866, -1.0736179750508374), mean=-1.7572495207388548, variance=1.1222779026962888, skewness=-0.6918317244030849, kurtosis=-1.5)\n",
      "[-4.384 -1.639 -1.767] DescribeResult(nobs=3, minmax=(-4.3842671501441313, -1.6393857266668597), mean=-2.5969818644062905, variance=2.3998815689673991, skewness=-0.7016876752653842, kurtosis=-1.5000000000000002)\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model, cross_validation\n",
    "linr = linear_model.LinearRegression()\n",
    "for reg in [etr,rtr,linr]:\n",
    "    scores = cross_validation.cross_val_score(reg, data, labels, scoring='mean_squared_error', n_jobs=4)\n",
    "    print scores, sp.stats.describe(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DescribeResult(nobs=3, minmax=(-2.9621733228118092, -0.95888797451615915), mean=-1.675997737110861, variance=1.2461649799446983, skewness=-0.6931416166960097, kurtosis=-1.4999999999999998)"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sp.stats.describe(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "metric_keys = list(weight_df.keys()[-n_metrics:])\n",
    "weight_df.set_index(metric_keys+['var','t'], inplace=True)\n",
    "# REMEMBER TO CHECK THIS WHEN DOING MULTIPLE RUNS (although technically it shouldn't matter....)\n",
    "weight_df.drop(['observer','run'], axis=1, inplace=True)\n",
    "# Sum for each run\n",
    "time_summed_weights = weight_df.groupby(level=list(weight_df.index.names[:-1])).sum().unstack('var')\n",
    "target_weights = time_summed_weights.xs(target,level='target', axis=1).fillna(0.0) # Nans map to no outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Single DataFrame of all features against known good\n",
    "var_weights = target_weights.apply(lambda s: s/target_weights.CombinedTrust, axis=0).dropna()\n",
    "known_good_features = \\\n",
    "    pd.concat([feature_extractor(s.reset_index(),var) for var,s  in var_weights.iteritems()],\n",
    "              keys=var_weights.keys(), names=['var','metric'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results handily confirm that there is a 'signature' of badmouthing as RandomFlatWalk was incorrectly configured. \n",
    "\n",
    "Need to: \n",
    "1. Perform multi-run tolerance analysis of metrics (i.e. turn the below into a boxplot)\n",
    "2. Perform cross correlation analysis on metrics across runs/behaviours (what metrics are redundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=known_good_features.unstack().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_=known_good_features.unstack().boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import operator\n",
    "\n",
    "def target_weight_feature_extractor(target_weights):\n",
    "    known_good_features_d = {}\n",
    "    for basekey in target_weights.keys(): # Parallelisable\n",
    "        print basekey\n",
    "        # Single DataFrame of all features against one behaviour\n",
    "        var_weights = target_weights.apply(lambda s: s/target_weights[basekey], axis=0).dropna()\n",
    "        known_good_features_d[basekey] = \\\n",
    "            pd.concat([feature_extractor(s.reset_index(),var) for var,s  in var_weights.iteritems()],\n",
    "                      keys=var_weights.keys(), names=['var','metric'])\n",
    "            \n",
    "    return known_good_features_d\n",
    "\n",
    "\n",
    "def dataframe_weight_filter(df, keys):\n",
    "    indexes = [(df.index.get_level_values(k)==0.0) for k in keys]\n",
    "    return df.loc[reduce(operator.and_,indexes)]\n",
    "\n",
    "phys_keys = ['INDD','INHD','Speed']\n",
    "comm_keys = ['ADelay','ARXP','ATXP','RXThroughput','TXThroughput','PLR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Comms Only Weights\n",
    "comms_target_weights=dataframe_weight_filter(target_weights,phys_keys)\n",
    "comms_target_weights.reset_index(level=phys_keys, drop=True, inplace=True)\n",
    "comms_features_d = target_weight_feature_extractor(comms_target_weights)\n",
    "for var,feat in comms_features_d.iteritems():\n",
    "    feat.unstack().plot(kind='bar', title=var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Phys Only Weights\n",
    "phys_target_weights=dataframe_weight_filter(target_weights,comm_keys)\n",
    "phys_target_weights.reset_index(level=comm_keys, drop=True, inplace=True)\n",
    "phys_features_d = target_weight_feature_extractor(phys_target_weights)\n",
    "for var,feat in phys_features_d.iteritems():\n",
    "    feat.unstack().plot(kind='bar', title=var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bounos.Analyses.Weight import summed_outliers_per_weight\n",
    "observer = 'Bravo'\n",
    "target = 'Alfa'\n",
    "n_nodes = 6\n",
    "n_metrics = 9\n",
    "\n",
    "results_path = \"/home/bolster/src/aietes/results/Malicious Behaviour Trust Comparison-2015-07-20-17-47-53\"\n",
    "\n",
    "with pd.get_store(results_path+\"/outliers.bkup.h5\") as store:\n",
    "    target_weights_dict = {}\n",
    "    for runkey in store.keys():\n",
    "        print runkey\n",
    "        target_weights_dict[runkey] = summed_outliers_per_weight(store.get(runkey), observer, n_metrics, target=target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_target_weights = pd.concat(target_weights_dict, names = ['run']+target_weights_dict[runkey].index.names)\n",
    "sorted_joined_target_weights = joined_target_weights.reset_index('run', drop=True).sort()\n",
    "joined_feats = target_weight_feature_extractor(sorted_joined_target_weights)\n",
    "\n",
    "\n",
    "alt_joined_feats= pd.concat(joined_feats, names=['base','comp','metric']).unstack('metric')[comm_keys+phys_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for k,g in alt_joined_feats.groupby(level='base'):\n",
    "    g.plot(kind='bar', title=k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_joined_target_weights.to_hdf('/dev/shm/target_weights_full.h5','target_weights_full')\n",
    "pd.get_store('/dev/shm/target_weights.h5').flush()\n",
    "matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "for var,feat in joined_feats.iteritems():\n",
    "    feat.unstack().plot(kind='bar', title=var)\n",
    "    break\n",
    "print var\n",
    "feat.unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feat_comps = {}\n",
    "for run,target_weights in joined_target_weights.groupby(level='run'): # Automatically drops the level\n",
    "    target_weights = target_weights.xs(run,level='run')\n",
    "    known_good_features_d = {}\n",
    "    for basekey in target_weights.keys(): # Parallelisable\n",
    "        print basekey\n",
    "        # Single DataFrame of all features against one behaviour\n",
    "        var_weights = target_weights.apply(lambda s: s/target_weights[basekey], axis=0).dropna()\n",
    "        known_good_features_d[basekey] = \\\n",
    "            pd.concat([feature_extractor(s.reset_index(),var) for var,s  in var_weights.iteritems()],\n",
    "                      keys=var_weights.keys(), names=['var','metric'])\n",
    "    feat_comps[run]=pd.DataFrame.from_dict(known_good_features_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_feat_comp = pd.concat(feat_comps)\n",
    "joined_feat_comp.unstack().swaplevel(0,1,axis=0).swaplevel(0,1,axis=1).sortlevel(axis=0).sortlevel(axis=1).reindex().groupby(level='var').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for run,target_weights in joined_target_weights.groupby(level='run'): # Automatically drops the level\n",
    "    print run\n",
    "    print target_weights.xs(run,level='run').head()\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "joined_feat_comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.seterr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.divide(np.ones(4),np.zeros(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
